{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the module\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from utils import *\n",
    "from file_reader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_project_id(file_name):\n",
    "    df = pd.read_csv(file_name, index_col=0)\n",
    "    #display(df.index)\n",
    "    return df.index\n",
    "    \n",
    "\n",
    "project_id_list = get_all_project_id('project_df_lib.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_lib_list(path,id_list,output_df):\n",
    "    lib_list=[]\n",
    "    df_header=['userID','itemID','rating']\n",
    "    df = pd.DataFrame(columns=df_header)\n",
    "    for i in id_list:\n",
    "        libs = read_file(path,i)\n",
    "        for j in libs:\n",
    "            df2 = pd.DataFrame({'userID': [i],\n",
    "                            'itemID': [j],\n",
    "                            'rating': [1]})\n",
    "            df = df.append(df2, ignore_index = True)\n",
    "\n",
    "        lib_list = lib_list+libs\n",
    "    #df.to_csv(output_df, index = False)\n",
    "    #display(df)    \n",
    "    lib_set = set(lib_list)\n",
    "    return lib_set,df\n",
    "\n",
    "def create_lib_df(path,id_list, output_df):\n",
    "    libs,temp_df = get_lib_list(path,id_list,output_df)\n",
    "    display(len(temp_df))\n",
    "    list_of_zeroes = [0]*(len(libs)*len(id_list))\n",
    "    index = pd.MultiIndex.from_product([id_list, libs], names = [\"userID\", \"itemID\"])\n",
    "    df = pd.DataFrame(index = index)\n",
    "    df['rating'] = list_of_zeroes\n",
    "    for i in range(0, len(temp_df)):\n",
    "        #print (temp_df.loc[i,'userID'])\n",
    "        df.loc[temp_df.loc[i,'userID'],temp_df.loc[i,'itemID']]=1\n",
    "    df.to_csv('surprise_libraries_df.csv')\n",
    "    display(df)\n",
    "\n",
    "create_lib_df('C:\\\\progetti\\\\low-code-ard-proj\\\\data-scraper\\\\code-snippets\\\\', project_id_list,output_df='temp_lib.csv')\n",
    "#get_lib_list('C:\\\\progetti\\\\low-code-ard-proj\\\\data-scraper\\\\code-snippets\\\\',project_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import get\n",
    "from surprise import Dataset,accuracy, NormalPredictor, Reader,SVD, KNNWithMeans,KNNBasic, KNNWithZScore\n",
    "from surprise.model_selection import cross_validate, train_test_split,KFold\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold=0.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for  uid,_, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    #print(user_est_true)\n",
    "    #display(user_est_true)\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        #print('relevant: ',n_rel)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        #print('recommended: ',n_rec_k)\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('temp_lib.csv')\n",
    " \n",
    "#display(df)\n",
    "\n",
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(0, 1))\n",
    "#print(df.columns)\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df[df.columns], reader)\n",
    "algo = KNNWithMeans(sim_options={\n",
    "        'name': 'msd',\n",
    "        'user_based': True,\n",
    "        })\n",
    "        #'min_support':5,\n",
    "        #'shrinkage':0\n",
    "\n",
    "# Retrieve the trainset.\n",
    "#trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "\n",
    "# define a cross-validation iterator\n",
    "kf = KFold(n_splits=2)\n",
    "\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=0.1)\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "#algo.fit(trainset)\n",
    "#predictions = algo.test(testset)\n",
    "# We can now use this dataset as we please, e.g. calling cross_validate\n",
    "#cross_validate(NormalPredictor(), data, cv=10)\n",
    "#cross_validate(algo, data, measures=[\"RMSE\", \"MAE\"], cv=10, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(real, lower_bound):\n",
    "    count = 0\n",
    "    for i in range(0,len(predictions)):\n",
    "        if(predictions[i][2]==real and predictions[i][3]>lower_bound):\n",
    "            count= count +1\n",
    "            print(count, predictions[i][2],predictions[i][3])\n",
    "       \n",
    "print_predictions(1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from surprise import accuracy, Dataset, SVD\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "df = pd.read_csv('surprise_libraries_df.csv')\n",
    " \n",
    "reader = Reader(rating_scale=(0, 1))\n",
    "\n",
    "data = Dataset.load_from_df(df[df.columns], reader)\n",
    "\n",
    "raw_ratings = data.raw_ratings\n",
    "\n",
    "# shuffle ratings if you want\n",
    "random.shuffle(raw_ratings)\n",
    "\n",
    "# A = 90% of the data, B = 10% of the data\n",
    "threshold = int(0.9 * len(raw_ratings))\n",
    "A_raw_ratings = raw_ratings[:threshold]\n",
    "B_raw_ratings = raw_ratings[threshold:]\n",
    "\n",
    "data.raw_ratings = A_raw_ratings  # data is now the set A\n",
    "\n",
    "# Select your best algo with grid search.\n",
    "print(\"Grid Search...\")\n",
    "param_grid = {\"n_epochs\": [5, 10], \"lr_all\": [0.002, 0.005]}\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=[\"rmse\"], cv=3)\n",
    "grid_search.fit(data)\n",
    "\n",
    "algo = grid_search.best_estimator[\"rmse\"]\n",
    "\n",
    "# retrain on the whole set A\n",
    "trainset = data.build_full_trainset()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Compute biased accuracy on A\n",
    "predictions = algo.test(trainset.build_testset())\n",
    "print(\"Biased accuracy on A,\", end=\"   \")\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Compute unbiased accuracy on B\n",
    "testset = data.construct_testset(B_raw_ratings)  # testset is now the set B\n",
    "predictions = algo.test(testset)\n",
    "print(\"Unbiased accuracy on B,\", end=\" \")\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dc0d7ba038b507a08c60add6cd202e09e1772ab4e5cb693912434db3e8a8d80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
